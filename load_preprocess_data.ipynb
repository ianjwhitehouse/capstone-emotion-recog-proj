{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can organize this notebook more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.load(\"openface_and_labels_by_frame.npz\")\n",
    "X, y = xy[\"x\"], xy[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (604901, 713)\n",
      "y shape: (604901, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713,)\n"
     ]
    }
   ],
   "source": [
    "print(X[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decide later exactly how much of the data ought to be used, but for the sake of testing things for now, I've truncated the dataset to something more manageable from a time perspective (sample a number (n_samples * m_percent) of random indices and extract those indices from the features/targets). I can perform a larger-in-scale training session over break on my GPU desktop (hopefully without interruption due to hardware limitations) once we're satisfied with the model parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_data(X, y, n_samples):\n",
    "    random_i = np.random.choice(len(X), size = n_samples)\n",
    "    X_trunc = X[random_i]\n",
    "    y_trunc = y[random_i]\n",
    "    return X_trunc, y_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6049"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(X.shape[0] * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_truncated, y_truncated = truncate_data(X, y, int(X.shape[0] * 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_truncated, y_truncated, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random X min: -154.36665\n",
      "random X max: 659.13336\n",
      "random y min: -0.33333334\n",
      "random y max: 0.0\n"
     ]
    }
   ],
   "source": [
    "X_sample = X[np.random.choice(len(X))]\n",
    "y_sample = y[np.random.choice(len(y))]\n",
    "print(\"random X min:\", np.min(X_sample))\n",
    "print(\"random X max:\", np.max(X_sample))\n",
    "print(\"random y min:\", np.min(y_sample))\n",
    "print(\"random y max:\", np.max(y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x min: -34065332.0\n",
      "x max: 48234530.0\n"
     ]
    }
   ],
   "source": [
    "print(\"x min:\", np.min(X))\n",
    "print(\"x max:\", np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,6].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y min: -3.0\n",
      "y max: 3.0\n",
      "y mean: 0.17519538\n",
      "y std: 0.56088483\n"
     ]
    }
   ],
   "source": [
    "print(\"y min:\", np.min(y))\n",
    "print(\"y max:\", np.max(y))\n",
    "print(\"y mean:\", np.mean(y[:,1:]))\n",
    "print(\"y std:\", np.std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min y, index 0 : -3.0\n",
      "max y, index 0 : 3.0 \n",
      "\n",
      "min y, index 1 : 0.0\n",
      "max y, index 1 : 3.0 \n",
      "\n",
      "min y, index 2 : 0.0\n",
      "max y, index 2 : 3.0 \n",
      "\n",
      "min y, index 3 : 0.0\n",
      "max y, index 3 : 3.0 \n",
      "\n",
      "min y, index 4 : 0.0\n",
      "max y, index 4 : 3.0 \n",
      "\n",
      "min y, index 5 : 0.0\n",
      "max y, index 5 : 3.0 \n",
      "\n",
      "min y, index 6 : 0.0\n",
      "max y, index 6 : 1.6666666 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(y.shape[1]):\n",
    "    print(\"min y, index\", i, \":\", np.min(y[:,i]))\n",
    "    print(\"max y, index\", i, \":\", np.max(y[:,i]), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per the paper, the integral [-3, 3] for the target values, as implied above (min/max) represents sentiment, with +3 being highly positive, -3 highly negative, 0 neutral. the mean value is also consistent with the distribution shown in the paper (pp 2240 fig 2), which shows that most annotations are neutral or weakly positive (0-1). do we need to balance the classes at some point? **note that the first index (i=0) of any given target array represents this integral\n",
    "\n",
    "the paper mentions a second integral [0,3] which measures the presence of emotion (0 = no presence, 3 = highest presence). it looks like indices 1-5 of the target arrays are of this integral.\n",
    "\n",
    "I'm not quite sure what the values in the final index (values range from [0, 2)) represent in the data. best assumption is that it's the same as the previous 5 values, because there are 6 emotions (happiness, sadness, anger, fear, disgust, surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (4839, 713)\n",
      "test: (1210, 713)\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", X_train.shape)\n",
    "print(\"test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n",
    "#   tf.keras.layers.Dense(712, activation='relu'),\n",
    "#   tf.keras.layers.Dense(712, activation='relu'),\n",
    "#   tf.keras.layers.Dense(356, activation='relu'),\n",
    "#   tf.keras.layers.Dense(178, activation='relu'),\n",
    "#   tf.keras.layers.Dense(7, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#   loss=loss,\n",
    "#   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "clf = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20646624,  1.11828705,  0.16974046, ...,  0.06052888,\n",
       "         0.17677589,  0.04378128],\n",
       "       [-0.00895974,  0.47875212,  0.20112771, ...,  0.05984263,\n",
       "         0.20490811,  0.0327238 ],\n",
       "       [ 0.38816177,  0.69448503,  0.12929643, ...,  0.05930335,\n",
       "         0.01839818,  0.04337723],\n",
       "       ...,\n",
       "       [-0.14418211,  0.21003477,  0.08317705, ...,  0.04908758,\n",
       "         0.09206759,  0.00165839],\n",
       "       [ 0.13683652,  0.56096793,  0.12085602, ...,  0.06018559,\n",
       "         0.17915968,  0.02938102],\n",
       "       [ 0.35032274,  0.91025027,  0.11037594, ...,  0.02353664,\n",
       "         0.06498344,  0.03875004]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.        ,  1.6666666 ,  0.        , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.33333334,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.3333334 ,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.        ,  0.        ,  0.6666667 , ...,  0.        ,\n",
       "         0.        ,  0.33333334],\n",
       "       [-2.6666667 ,  0.        ,  0.        , ...,  0.        ,\n",
       "         2.        ,  0.        ],\n",
       "       [ 1.6666666 ,  1.6666666 ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
